{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked LSTMs for Time Series Classification with TensorFlow\n",
    "02_stacked_lstm_with_feature_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now build a slightly deeper model by stacking two LSTM layers using the EOD stock price data. Furthermore, we will include features that are not sequential in nature, namely indicator variables for identifying the equity and the month."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 00:06:17.343077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, concatenate, Embedding, Reshape, BatchNormalization\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 00:06:21.494504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-01 00:06:21.542457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-01 00:06:21.542523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path('results', 'lstm_embeddings')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data produced by the notebook [build_dataset](00_build_dataset.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf('lstm_data.h5', 'returns_weekly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 354903 entries, ('A', Timestamp('2007-01-07 00:00:00')) to ('ZION', Timestamp('2023-02-26 00:00:00'))\n",
      "Data columns (total 67 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   fwd_returns  354903 non-null  float64\n",
      " 1   1            354903 non-null  float64\n",
      " 2   2            354903 non-null  float64\n",
      " 3   3            354903 non-null  float64\n",
      " 4   4            354903 non-null  float64\n",
      " 5   5            354903 non-null  float64\n",
      " 6   6            354903 non-null  float64\n",
      " 7   7            354903 non-null  float64\n",
      " 8   8            354903 non-null  float64\n",
      " 9   9            354903 non-null  float64\n",
      " 10  10           354903 non-null  float64\n",
      " 11  11           354903 non-null  float64\n",
      " 12  12           354903 non-null  float64\n",
      " 13  13           354903 non-null  float64\n",
      " 14  14           354903 non-null  float64\n",
      " 15  15           354903 non-null  float64\n",
      " 16  16           354903 non-null  float64\n",
      " 17  17           354903 non-null  float64\n",
      " 18  18           354903 non-null  float64\n",
      " 19  19           354903 non-null  float64\n",
      " 20  20           354903 non-null  float64\n",
      " 21  21           354903 non-null  float64\n",
      " 22  22           354903 non-null  float64\n",
      " 23  23           354903 non-null  float64\n",
      " 24  24           354903 non-null  float64\n",
      " 25  25           354903 non-null  float64\n",
      " 26  26           354903 non-null  float64\n",
      " 27  27           354903 non-null  float64\n",
      " 28  28           354903 non-null  float64\n",
      " 29  29           354903 non-null  float64\n",
      " 30  30           354903 non-null  float64\n",
      " 31  31           354903 non-null  float64\n",
      " 32  32           354903 non-null  float64\n",
      " 33  33           354903 non-null  float64\n",
      " 34  34           354903 non-null  float64\n",
      " 35  35           354903 non-null  float64\n",
      " 36  36           354903 non-null  float64\n",
      " 37  37           354903 non-null  float64\n",
      " 38  38           354903 non-null  float64\n",
      " 39  39           354903 non-null  float64\n",
      " 40  40           354903 non-null  float64\n",
      " 41  41           354903 non-null  float64\n",
      " 42  42           354903 non-null  float64\n",
      " 43  43           354903 non-null  float64\n",
      " 44  44           354903 non-null  float64\n",
      " 45  45           354903 non-null  float64\n",
      " 46  46           354903 non-null  float64\n",
      " 47  47           354903 non-null  float64\n",
      " 48  48           354903 non-null  float64\n",
      " 49  49           354903 non-null  float64\n",
      " 50  50           354903 non-null  float64\n",
      " 51  51           354903 non-null  float64\n",
      " 52  52           354903 non-null  float64\n",
      " 53  label        354903 non-null  int64  \n",
      " 54  ticker       354903 non-null  int64  \n",
      " 55  month_1      354903 non-null  uint8  \n",
      " 56  month_2      354903 non-null  uint8  \n",
      " 57  month_3      354903 non-null  uint8  \n",
      " 58  month_4      354903 non-null  uint8  \n",
      " 59  month_5      354903 non-null  uint8  \n",
      " 60  month_6      354903 non-null  uint8  \n",
      " 61  month_7      354903 non-null  uint8  \n",
      " 62  month_8      354903 non-null  uint8  \n",
      " 63  month_9      354903 non-null  uint8  \n",
      " 64  month_10     354903 non-null  uint8  \n",
      " 65  month_11     354903 non-null  uint8  \n",
      " 66  month_12     354903 non-null  uint8  \n",
      "dtypes: float64(53), int64(2), uint8(12)\n",
      "memory usage: 154.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data['ticker'] = pd.factorize(data.index.get_level_values('ticker'))[0]\n",
    "data['month'] = data.index.get_level_values('date').month\n",
    "data = pd.get_dummies(data, columns=['month'], prefix='month')\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "To respect the time series nature of the data, we set aside the data at the end of the sample as hold-out or test set. More specifically, we'll use the data for 2020-2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=52\n",
    "sequence = list(range(1, window_size+1))\n",
    "ticker = 1\n",
    "months = 12\n",
    "n_tickers = data.ticker.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.drop('fwd_returns', axis=1).loc[idx[:, :'2019'], :]\n",
    "test_data = data.drop('fwd_returns', axis=1).loc[idx[:, '2020'],:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each train and test dataset, we generate a list with three input arrays containing the return series, the stock ticker (converted to integer values), and the month (as an integer), as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(285438, 52, 1), (285438,), (285438, 12)], (285438,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [\n",
    "    train_data.loc[:, sequence].values.reshape(-1, window_size , 1),\n",
    "    train_data.ticker,\n",
    "    train_data.filter(like='month')\n",
    "]\n",
    "y_train = train_data.label\n",
    "[x.shape for x in X_train], y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(21892, 52, 1), (21892,), (21892, 12)], (21892,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the last year for testing\n",
    "X_test = [\n",
    "    test_data.loc[:, list(range(1, window_size+1))].values.reshape(-1, window_size , 1),\n",
    "    test_data.ticker,\n",
    "    test_data.filter(like='month')\n",
    "]\n",
    "y_test = test_data.label\n",
    "[x.shape for x in X_test], y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define the Model Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functional API of Keras makes it easy to design architectures with multiple inputs and outputs. This example illustrates a network with three inputs, as follows:\n",
    "\n",
    "- A two stacked LSTM layers with 25 and 10 units respectively\n",
    "- An embedding layer that learns a 10-dimensional real-valued representation of the equities\n",
    "- A one-hot encoded representation of the month\n",
    "\n",
    "This can be constructed using just a few lines - see e.g., \n",
    "- the [general Keras documentation](https://keras.io/getting-started/sequential-model-guide/), \n",
    "- the [LSTM documentation](https://keras.io/layers/recurrent/).\n",
    "\n",
    "Make sure you are initializing your optimizer given the [keras-recommended approach for RNNs](https://keras.io/optimizers/) \n",
    "\n",
    "We begin by defining the three inputs with their respective shapes, as described here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = Input(shape=(window_size, n_features),\n",
    "                name='Returns')\n",
    "\n",
    "tickers = Input(shape=(1,),\n",
    "                name='Tickers')\n",
    "\n",
    "months = Input(shape=(12,),\n",
    "               name='Months')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define stacked LSTM layers, we set the `return_sequences` keyword to `True`. This ensures that the first layer produces an output that conforms to the expected three-dimensional input format. Note that we also use dropout regularization and how the functional API passes the tensor outputs from one layer to the subsequent layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1_units = 25\n",
    "lstm2_units = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 00:17:28.579447: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-01 00:17:28.580899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-01 00:17:28.580985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-01 00:17:28.581025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-01 00:17:30.291034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-01 00:17:30.291141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-01 00:17:30.291152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-01 00:17:30.291193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-01 00:17:30.291406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "lstm1 = LSTM(units=lstm1_units, \n",
    "             input_shape=(window_size, \n",
    "                          n_features), \n",
    "             name='LSTM1', \n",
    "             dropout=.2,\n",
    "             return_sequences=True)(returns)\n",
    "\n",
    "lstm_model = LSTM(units=lstm2_units, \n",
    "             dropout=.2,\n",
    "             name='LSTM2')(lstm1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer requires the `input_dim` keyword, which defines how many embeddings the layer will learn, the `output_dim` keyword, which defines the size of the embedding, and the `input_length` keyword to set the number of elements passed to the layer (here only one ticker per sample). \n",
    "\n",
    "To combine the embedding layer with the LSTM layer and the months input, we need to reshape (or flatten) it, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_embedding = Embedding(input_dim=n_tickers, \n",
    "                             output_dim=5, \n",
    "                             input_length=1)(tickers)\n",
    "ticker_embedding = Reshape(target_shape=(5,))(ticker_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Model components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can concatenate the three tensors and add fully-connected layers to learn a mapping from these learned time series, ticker, and month indicators to the outcome, a positive or negative return in the following week, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = concatenate([lstm_model, \n",
    "                      ticker_embedding, \n",
    "                      months], name='Merged')\n",
    "\n",
    "bn = BatchNormalization()(merged)\n",
    "hidden_dense = Dense(10, name='FC1')(bn)\n",
    "\n",
    "output = Dense(1, name='Output', activation='sigmoid')(hidden_dense)\n",
    "\n",
    "rnn = Model(inputs=[returns, tickers, months], outputs=output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary lays out this slightly more sophisticated architecture with 29,371????? parameters, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Returns (InputLayer)           [(None, 52, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Tickers (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " LSTM1 (LSTM)                   (None, 52, 25)       2700        ['Returns[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 5)         2105        ['Tickers[0][0]']                \n",
      "                                                                                                  \n",
      " LSTM2 (LSTM)                   (None, 10)           1440        ['LSTM1[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 5)            0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " Months (InputLayer)            [(None, 12)]         0           []                               \n",
      "                                                                                                  \n",
      " Merged (Concatenate)           (None, 27)           0           ['LSTM2[0][0]',                  \n",
      "                                                                  'reshape[0][0]',                \n",
      "                                                                  'Months[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 27)          108         ['Merged[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " FC1 (Dense)                    (None, 10)           280         ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " Output (Dense)                 (None, 1)            11          ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,644\n",
      "Trainable params: 6,590\n",
      "Non-trainable params: 54\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model to compute a custom auc metric as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.RMSprop(lr=0.001,\n",
    "                                        rho=0.9,\n",
    "                                        epsilon=1e-08,\n",
    "                                        decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy', \n",
    "                     tf.keras.metrics.AUC(name='AUC')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_path = (results_path / 'lstm.classification.h5').as_posix()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=lstm_path,\n",
    "                               verbose=1,\n",
    "                               monitor='val_AUC',\n",
    "                               mode='max',\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_AUC', \n",
    "                              patience=5,\n",
    "                              restore_best_weights=True,\n",
    "                              mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 00:18:38.100501: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 59371104 exceeds 10% of free system memory.\n",
      "2023-03-01 00:18:38.202000: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 59371104 exceeds 10% of free system memory.\n",
      "2023-03-01 00:18:42.292439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8920/8920 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.5381 - AUC: 0.5279\n",
      "Epoch 1: val_AUC improved from -inf to 0.48637, saving model to results/lstm_embeddings/lstm.classification.h5\n",
      "8920/8920 [==============================] - 207s 23ms/step - loss: 0.6900 - accuracy: 0.5381 - AUC: 0.5279 - val_loss: 0.6959 - val_accuracy: 0.5362 - val_AUC: 0.4864\n",
      "Epoch 2/50\n",
      "8919/8920 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.5425 - AUC: 0.5333\n",
      "Epoch 2: val_AUC improved from 0.48637 to 0.51227, saving model to results/lstm_embeddings/lstm.classification.h5\n",
      "8920/8920 [==============================] - 204s 23ms/step - loss: 0.6881 - accuracy: 0.5425 - AUC: 0.5333 - val_loss: 0.6905 - val_accuracy: 0.5381 - val_AUC: 0.5123\n",
      "Epoch 3/50\n",
      "8920/8920 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.5455 - AUC: 0.5391\n",
      "Epoch 3: val_AUC did not improve from 0.51227\n",
      "8920/8920 [==============================] - 214s 24ms/step - loss: 0.6874 - accuracy: 0.5455 - AUC: 0.5391 - val_loss: 0.6936 - val_accuracy: 0.5427 - val_AUC: 0.4926\n",
      "Epoch 4/50\n",
      "8920/8920 [==============================] - ETA: 0s - loss: 0.6871 - accuracy: 0.5458 - AUC: 0.5413\n",
      "Epoch 4: val_AUC did not improve from 0.51227\n",
      "8920/8920 [==============================] - 217s 24ms/step - loss: 0.6871 - accuracy: 0.5458 - AUC: 0.5413 - val_loss: 0.6927 - val_accuracy: 0.5303 - val_AUC: 0.4893\n",
      "Epoch 5/50\n",
      "8918/8920 [============================>.] - ETA: 0s - loss: 0.6866 - accuracy: 0.5477 - AUC: 0.5445\n",
      "Epoch 5: val_AUC did not improve from 0.51227\n",
      "8920/8920 [==============================] - 212s 24ms/step - loss: 0.6866 - accuracy: 0.5478 - AUC: 0.5445 - val_loss: 0.6981 - val_accuracy: 0.4894 - val_AUC: 0.4858\n",
      "Epoch 6/50\n",
      "8918/8920 [============================>.] - ETA: 0s - loss: 0.6866 - accuracy: 0.5474 - AUC: 0.5445\n",
      "Epoch 6: val_AUC did not improve from 0.51227\n",
      "8920/8920 [==============================] - 213s 24ms/step - loss: 0.6866 - accuracy: 0.5474 - AUC: 0.5445 - val_loss: 0.6951 - val_accuracy: 0.5298 - val_AUC: 0.4894\n",
      "Epoch 7/50\n",
      "8919/8920 [============================>.] - ETA: 0s - loss: 0.6867 - accuracy: 0.5473 - AUC: 0.5443\n",
      "Epoch 7: val_AUC improved from 0.51227 to 0.51495, saving model to results/lstm_embeddings/lstm.classification.h5\n",
      "8920/8920 [==============================] - 216s 24ms/step - loss: 0.6867 - accuracy: 0.5473 - AUC: 0.5443 - val_loss: 0.6916 - val_accuracy: 0.5321 - val_AUC: 0.5149\n",
      "Epoch 8/50\n",
      "8920/8920 [==============================] - ETA: 0s - loss: 0.6869 - accuracy: 0.5472 - AUC: 0.5429\n",
      "Epoch 8: val_AUC improved from 0.51495 to 0.52266, saving model to results/lstm_embeddings/lstm.classification.h5\n",
      "8920/8920 [==============================] - 213s 24ms/step - loss: 0.6869 - accuracy: 0.5472 - AUC: 0.5429 - val_loss: 0.6889 - val_accuracy: 0.5433 - val_AUC: 0.5227\n",
      "Epoch 9/50\n",
      "3181/8920 [=========>....................] - ETA: 3:12 - loss: 0.6866 - accuracy: 0.5456 - AUC: 0.5462"
     ]
    }
   ],
   "source": [
    "training = rnn.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=50,\n",
    "                   batch_size=32,\n",
    "                   validation_data=(X_test, y_test),\n",
    "                   callbacks=[early_stopping, checkpointer],\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = pd.DataFrame(training.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_metric(m):\n",
    "    return m.split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(18,4))\n",
    "for i, (metric, hist) in enumerate(loss_history.groupby(which_metric, axis=1)):\n",
    "    hist.plot(ax=axes[i], title=metric)\n",
    "    axes[i].legend(['Training', 'Validation'])\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'lstm_stacked_classification', dpi=300);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = pd.Series(rnn.predict(X_test).squeeze(), index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_score=test_predict, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((test_predict>.5) == y_test).astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(test_predict, y_test)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "434ef85c3fb5a296e505a484f164c3e4672a7a65819d5b3b14e63d027f7747a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
