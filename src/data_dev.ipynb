{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importation, Cleaning, and Transformation\n",
    "\n",
    "Data Source: https://eodhistoricaldata.com/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib as jl\n",
    "import datetime as dt\n",
    "import requests\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from ta import add_all_ta_features\n",
    "from ta.trend import MACD\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.volume import VolumeWeightedAveragePrice\n",
    "from ta.momentum import StochRSIIndicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load API keys from .env file\n",
    "load_dotenv()\n",
    "TRADIER_TOKEN = os.getenv('TRADIER_TOKEN')\n",
    "EOD_TOKEN = os.getenv('EOD_TOKEN')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_etf_tickers(): \n",
    "    \n",
    "    # pulls all tickers of ETFs on NYSE or NASDAQ\n",
    "\n",
    "    r = requests.get('https://eodhistoricaldata.com/api/exchange-symbol-list/US', \n",
    "        params={'api_token': EOD_TOKEN, 'fmt': 'json'}\n",
    "        )\n",
    "    data = r.json()\n",
    "    r.close()\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df[\n",
    "        (df.Type == 'ETF') &\n",
    "        ((df.Exchange == 'NYSE ARCA') |\n",
    "        (df.Exchange == 'NASDAQ'))\n",
    "        ]\n",
    "\n",
    "    df.index = df.Code\n",
    "    df.drop('Code', axis = 1, inplace=True)\n",
    "    ticker_list = list(df.index)\n",
    "    return ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get etf tickers\n",
    "# tickers = get_etf_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ticker list to csv\n",
    "# df = pd.DataFrame(tickers)\n",
    "# df.to_csv('data/tickers.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Historical Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_price(tickers, data_type):\n",
    "\n",
    "    # pulls historical daily or intraday OLHC prices and volume\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    for i in range(len(tickers)): \n",
    "\n",
    "        r = requests.get('https://eodhistoricaldata.com/api' + '/' + data_type + '/' + tickers[i] + '.US', \n",
    "            params={'api_token': EOD_TOKEN, 'fmt': 'json'}\n",
    "            )\n",
    "        data = r.json()\n",
    "        r.close()\n",
    "\n",
    "        # ADD PROGRESS BAR\n",
    "        \n",
    "        # remove unecessary loop\n",
    "        for symbol in tickers:\n",
    "            d[symbol] = pd.DataFrame.from_records(data)\n",
    "\n",
    "            indicator_macd = MACD(close = d[symbol]['close'], window_slow=26, window_fast=12, window_sign=9, fillna=True)\n",
    "            d[symbol]['macd'] = indicator_macd.macd()\n",
    "            d[symbol]['macd_diff'] = indicator_macd.macd_diff()\n",
    "            d[symbol]['macd_signal'] = indicator_macd.macd_signal()\n",
    "\n",
    "            indicator_bb = BollingerBands(close = d[symbol]['close'], window=20, window_dev=2, fillna=True)\n",
    "            d[symbol]['bb_mavg'] = indicator_bb.bollinger_mavg()\n",
    "            d[symbol]['bb_hband'] = indicator_bb.bollinger_hband()\n",
    "            d[symbol]['bb_lband'] = indicator_bb.bollinger_lband()\n",
    "            d[symbol]['bb_hband_ind'] = indicator_bb.bollinger_hband_indicator()\n",
    "            d[symbol]['bb_lband_ind'] = indicator_bb.bollinger_lband_indicator()\n",
    "\n",
    "            indicator_vwap = VolumeWeightedAveragePrice(\n",
    "                high = d[symbol]['high'],\n",
    "                low = d[symbol]['low'],\n",
    "                close = d[symbol]['close'],\n",
    "                volume = d[symbol]['volume'],\n",
    "                window=14, fillna=True)\n",
    "            d[symbol]['vwap'] = indicator_vwap.volume_weighted_average_price()\n",
    "\n",
    "            indicator_stochrsi = StochRSIIndicator(close = d[symbol]['close'], window=14, smooth1=3, smooth2=3, fillna=True)\n",
    "            d[symbol]['stoch_rsi'] = indicator_stochrsi.stochrsi()\n",
    "            d[symbol]['stochrsi_d'] = indicator_stochrsi.stochrsi_d()\n",
    "            d[symbol]['stochrsi_k'] = indicator_stochrsi.stochrsi_k()\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder tickers\n",
    "tickers = ['SPXL', 'SPXS']\n",
    "\n",
    "# get intraday prices | dictionary of dataframes\n",
    "intraday_data = get_historical_price(tickers, 'intraday')\n",
    "\n",
    "# get daily prices | dictionary of dataframes\n",
    "daily_data = get_historical_price(tickers, 'eod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat intraday dataframes together and convert to dask df\n",
    "intraday_df = pd.concat(intraday_data.values(), axis=1, keys=intraday_data.keys())\n",
    "intraday_dask_df = dd.from_pandas(intraday_df, npartitions=6)\n",
    "\n",
    "# concat daily dataframes together and convert to dask df\n",
    "daily_df = pd.concat(daily_data.values(), axis=1, keys=daily_data.keys())\n",
    "daily_dask_df = dd.from_pandas(daily_df, npartitions=6)\n",
    "\n",
    "#daily_df.to_csv('data/test_daily_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ETF Fundementals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fundementals(tickers): \n",
    "\n",
    "    # pulls fundementals and wrangles data into multiple dfs\n",
    "\n",
    "    raw_data = {}\n",
    "    single_ticker_clean_data = {}\n",
    "    all_clean_data = {}\n",
    "\n",
    "\n",
    "    columns = ['ISIN', 'Company_Name', 'Company_URL', 'ETF_URL', 'Domicile',\n",
    "        'Index_Name', 'Yield', 'Dividend_Paying_Frequency', 'Inception_Date',\n",
    "        'Max_Annual_Mgmt_Charge', 'Ongoing_Charge', 'Date_Ongoing_Charge',\n",
    "        'NetExpenseRatio', 'AnnualHoldingsTurnover', 'TotalAssets', 'Holdings_Count',\n",
    "        'Average_Mkt_Cap_Mil']\n",
    "\n",
    "    for i in range(len(tickers)): \n",
    "        \n",
    "        r = requests.get('https://eodhistoricaldata.com/api/fundamentals/' + tickers[i] + '.US', \n",
    "            params={'api_token': '63dc0e2f4efc43.34327983', 'fmt': 'json'}\n",
    "            )\n",
    "        data = r.json()\n",
    "        \n",
    "        r.close()\n",
    "\n",
    "        # place raw data for each ticker inside nested dict\n",
    "        raw_data[tickers[i]] = data    \n",
    "\n",
    "        a = pd.Series(raw_data[tickers[i]]['General'])\n",
    "\n",
    "        b = pd.Series([raw_data[tickers[i]]['ETF_Data'][name] for name in columns],\n",
    "            index=[name for name in columns])\n",
    "\n",
    "        # figure out fix\n",
    "        # c = pd.DataFrame([d[tickers[i]]['ETF_Data']['Market_Capitalisation']])#,\n",
    "        # index = [d[tickers[i]]['ETF_Data']['Market_Capitalisation'].keys()])\n",
    "        # c = pd.Series(d[symbol]['ETF_Data']['Market_Capitalisation'],\n",
    "        #  index = ['Market_Capitalisation'])\n",
    "\n",
    "\n",
    "        c = pd.Series(raw_data[tickers[i]]['ETF_Data']['MorningStar'])\n",
    "\n",
    "        # figure out fix\n",
    "        # d = pd.DataFrame(d[tickers[i]]['ETF_Data']['Performance'].items(),\n",
    "        #     index = d[tickers[i]]['ETF_Data']['Performance'].keys()).drop(0, axis=1)\n",
    "        # d = pd.DataFrame(d[symbol]['ETF_Data']['Performance'].items(),\n",
    "        #  index = d[symbol]['ETF_Data']['Performance'].keys()).drop(0, axis=1)\n",
    "        \n",
    "        # e = pd.Series(d[symbol]['Technicals'])\n",
    "        # e = pd.Series(d[tickers[i]]['Technicals'].items(), \n",
    "        #     index = d[tickers[i]]['Technicals'].keys()).drop(0, axis=1)\n",
    "\n",
    "        single_ticker_clean_data['general'] = pd.DataFrame(pd.concat([a, b, c]), columns = ['data'])\n",
    "        single_ticker_clean_data['asset_allocation'] = pd.DataFrame(raw_data[tickers[i]]['ETF_Data']['Asset_Allocation'])\n",
    "        # fix this\n",
    "        region_weights_df = pd.DataFrame(raw_data[tickers[i]]['ETF_Data']['World_Regions'])\n",
    "        sector_weights_df = pd.DataFrame(raw_data[tickers[i]]['ETF_Data']['Sector_Weights'])\n",
    "        single_ticker_clean_data['fixed_income'] = pd.DataFrame(raw_data[tickers[i]]['ETF_Data']['Fixed_Income'])\n",
    "        single_ticker_clean_data['top_10_holdings'] = pd.DataFrame(raw_data[tickers[i]]['ETF_Data']['Top_10_Holdings'].values(), index = raw_data[tickers[i]]['ETF_Data']['Top_10_Holdings'].keys())\n",
    "        single_ticker_clean_data['holdings'] = pd.DataFrame(raw_data[tickers[i]]['ETF_Data']['Holdings'].values(), index = raw_data[tickers[i]]['ETF_Data']['Holdings'].keys())\n",
    "        single_ticker_clean_data['valuations_growth'] = pd.DataFrame(raw_data[tickers[i]]['ETF_Data']['Valuations_Growth']).T\n",
    "\n",
    "        # fix this\n",
    "        placeholder = {}\n",
    "        placeholder['region_weights'] = region_weights_df\n",
    "        placeholder['sector_weights'] = sector_weights_df\n",
    "        weights_df = pd.concat(placeholder.values(), axis=1, keys=placeholder.keys())\n",
    "        single_ticker_clean_data['weights'] = weights_df.copy()\n",
    "\n",
    "        all_clean_data[tickers[i]] = single_ticker_clean_data\n",
    "\n",
    "\n",
    "    return all_clean_data, single_ticker_clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # placeholder tickers\n",
    "tickers = ['SPY', 'QQQ']\n",
    "\n",
    "# # get fundemental data\n",
    "fundemental_data, single_ticker = get_fundementals(tickers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Dev\n",
    "- then develop flow for analysis on whiteboard, use paper as resource"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Stock Fundementals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_fundmentals(tickers): \n",
    "\n",
    "    raw_data = {}\n",
    "    single_ticker_clean_data = {}\n",
    "    all_clean_data = {}\n",
    "\n",
    "    columns = ['General', 'Highlights']\n",
    "\n",
    "    for i in range(len(tickers)): \n",
    "        print(tickers[i])\n",
    "\n",
    "        r = requests.get('https://eodhistoricaldata.com/api/fundamentals/' + tickers[i] + '.US', \n",
    "            params={'api_token': '63dc0e2f4efc43.34327983', 'fmt': 'json'}\n",
    "            )\n",
    "        data = r.json()\n",
    "        \n",
    "        r.close()\n",
    "\n",
    "        raw_data[tickers[i]] = data\n",
    "        \n",
    "        Officers = raw_data[tickers[i]]['General'].pop('Officers', None)\n",
    "        Listings = raw_data[tickers[i]]['General'].pop('Listings', None)\n",
    "        AddressData = raw_data[tickers[i]]['General'].pop('AddressData', None)\n",
    "        NumberDividendsByYear = raw_data[tickers[i]]['SplitsDividends'].pop('NumberDividendsByYear', None)\n",
    "\n",
    "        columns = ['General', 'Highlights', 'Valuation', 'SharesStats',\n",
    "            'Technicals','SplitsDividends', 'AnalystRatings']\n",
    "        for name in columns:\n",
    "            single_ticker_clean_data[name] = pd.Series(raw_data[tickers[i]][name])\n",
    "\n",
    "        columns = ['Institutions', 'Funds']\n",
    "        for name in columns:\n",
    "            single_ticker_clean_data[name] = pd.DataFrame(\n",
    "                raw_data[tickers[i]]['Holders'][name]).T.set_index(['name'])\n",
    "        single_ticker_clean_data['Holders'] = pd.concat([single_ticker_clean_data['Institutions'], single_ticker_clean_data['Funds']],\n",
    "            keys=['Institutions','Funds'])\n",
    "        Institutions = single_ticker_clean_data.pop('Institutions', None)\n",
    "        Funds = single_ticker_clean_data.pop('Funds', None)\n",
    "\n",
    "        single_ticker_clean_data['InsiderTransactions'] = pd.DataFrame(\n",
    "            raw_data[tickers[i]]['InsiderTransactions']).T.set_index(['date'])\n",
    "\n",
    "        columns = ['annual', 'quarterly']\n",
    "        for name in columns:\n",
    "            single_ticker_clean_data[name] = pd.DataFrame(\n",
    "                raw_data[tickers[i]]['outstandingShares'][name]).T.set_index(['dateFormatted'])\n",
    "        single_ticker_clean_data['outstandingShares'] = single_ticker_clean_data['quarterly'].join(\n",
    "            single_ticker_clean_data['annual'], lsuffix='_quarterly', rsuffix='_annual')\n",
    "        Institutions = single_ticker_clean_data.pop('quarterly', None)\n",
    "        Funds = single_ticker_clean_data.pop('annual', None)\n",
    "\n",
    "        # join on date\n",
    "        columns = ['History', 'Trend', 'Annual']\n",
    "        for name in columns:\n",
    "            single_ticker_clean_data[name] = pd.DataFrame(raw_data[tickers[i]]['Earnings'][name]).T\n",
    "        single_ticker_clean_data['Earnings'] = pd.concat([single_ticker_clean_data['History'], \n",
    "            single_ticker_clean_data['Trend'], single_ticker_clean_data['Annual']],\n",
    "            keys=['History', 'Trend', 'Annual'], axis=1)\n",
    "        History = single_ticker_clean_data.pop('History', None)\n",
    "        Trend = single_ticker_clean_data.pop('Trend', None)\n",
    "        Annual = single_ticker_clean_data.pop('Annual', None)\n",
    "\n",
    "        statements = ['Balance_Sheet', 'Income_Statement', 'Cash_Flow']\n",
    "        period = ['quarterly', 'yearly']\n",
    "\n",
    "        for statement in statements:\n",
    "            for time in period:\n",
    "                # join on date\n",
    "                single_ticker_clean_data[statement + time] = pd.DataFrame(\n",
    "                    raw_data[tickers[i]]['Financials'][statement][time]).T\n",
    "\n",
    "        # single_ticker_clean_data['Balance_Sheet'] = fundemental_data['Balance_Sheetquarterly'].join(\n",
    "        #     fundemental_data['Balance_Sheetyearly'], lsuffix='_quarterly', rsuffix='_annual')\n",
    "        single_ticker_clean_data['Balance_Sheet'] = pd.concat([single_ticker_clean_data['Balance_Sheetquarterly'], \n",
    "            single_ticker_clean_data['Balance_Sheetyearly']],\n",
    "            keys=['Quarterly', 'Yearly'], axis=1)\n",
    "        Balance_Sheetyearly = single_ticker_clean_data.pop('Balance_Sheetyearly', None)\n",
    "        Balance_Sheetquarterly = single_ticker_clean_data.pop('Balance_Sheetquarterly', None)\n",
    "\n",
    "        single_ticker_clean_data['Income_Statement'] = pd.concat([single_ticker_clean_data['Income_Statementquarterly'], \n",
    "            single_ticker_clean_data['Income_Statementyearly']],\n",
    "            keys=['Quarterly', 'Yearly'], axis=1)\n",
    "        Income_Statementyearly = single_ticker_clean_data.pop('Income_Statementyearly', None)\n",
    "        Income_Statementquarterly = single_ticker_clean_data.pop('Income_Statementquarterly', None)\n",
    "\n",
    "        \n",
    "        single_ticker_clean_data['Cash_Flow'] = pd.concat([single_ticker_clean_data['Cash_Flowquarterly'], \n",
    "            single_ticker_clean_data['Cash_Flowyearly']],\n",
    "            keys=['Quarterly', 'Yearly'], axis=1)\n",
    "        Cash_Flowyearly = single_ticker_clean_data.pop('Cash_Flowyearly', None)\n",
    "        Cash_Flowquarterly = single_ticker_clean_data.pop('Cash_Flowquarterly', None)\n",
    "\n",
    "        all_clean_data[tickers[i]] = single_ticker_clean_data\n",
    "\n",
    "\n",
    "\n",
    "    return all_clean_data, single_ticker_clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT\n",
      "AMZN\n"
     ]
    }
   ],
   "source": [
    "tickers = ['MSFT', 'AMZN']\n",
    "fundemental_data, single_ticker  = get_stock_fundmentals(tickers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honors-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cec9fab241fb47ace68fb2f1a373c81bce58fef94d497d1b6d3a75b16829e6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
